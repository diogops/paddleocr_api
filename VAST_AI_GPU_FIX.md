# Solu√ß√£o para Erro cuDNN no vast.ai (CUDA 11.8)

## üìã Resumo do Problema

Voc√™ est√° enfrentando um **segmentation fault** ao tentar rodar o PaddleOCR no vast.ai com GPU. O erro ocorre porque:

1. **cuDNN ausente**: O arquivo `/usr/local/cuda/lib64/libcudnn.so` n√£o est√° sendo encontrado
2. **Fallback quebrado**: Quando tenta usar CPU como fallback, o PaddlePaddle GPU causa segfault
3. **Imagem base incompleta**: A imagem `nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04` pode n√£o ter todos os componentes cuDNN

```
Error Message:
W1111 17:02:31.435693    46 dynamic_loader.cc:314] The third-party dynamic library (libcudnn.so) that Paddle depends on is not configured correctly.
  (error code is /usr/local/cuda/lib64/libcudnn.so: cannot open shared object file: No such file or directory)

FatalError: `Segmentation fault` is detected by the operating system.
```

---

## ‚úÖ Solu√ß√µes Dispon√≠veis

Criamos **2 Dockerfiles corrigidos** + **server.py melhorado**:

### **Solu√ß√£o 1: Dockerfile.gpu** (Recomendada para produ√ß√£o)
Usa imagem `devel` que tem cuDNN completo instalado

### **Solu√ß√£o 2: Dockerfile.gpu-fixed** (Mais robusta)
Usa imagem `runtime` mas instala cuDNN manualmente e cria links simb√≥licos

### **Solu√ß√£o 3: server.py melhorado** (J√° aplicada)
Detecta melhor GPU e evita segfault no fallback para CPU

---

## üöÄ Como Usar (Passo a Passo)

### **Op√ß√£o A: Dockerfile.gpu (RECOMENDADA)**

```bash
# 1. Build da imagem usando Dockerfile.gpu
docker build -f Dockerfile.gpu -t paddleocr-api:gpu .

# 2. Testar localmente primeiro (opcional)
docker run --gpus all -p 8000:8000 paddleocr-api:gpu

# 3. Verificar se GPU est√° funcionando
curl http://localhost:8000/health

# Voc√™ deve ver logs como:
# ‚úÖ GPU detectada! 1 GPU(s) dispon√≠vel(is)
# ‚úÖ CUDA completamente funcional - usando GPU
```

### **Op√ß√£o B: Dockerfile.gpu-fixed (MAIS ROBUSTA)**

```bash
# 1. Build usando Dockerfile.gpu-fixed
docker build -f Dockerfile.gpu-fixed -t paddleocr-api:gpu-fixed .

# 2. Run
docker run --gpus all -p 8000:8000 paddleocr-api:gpu-fixed
```

---

## üîç Principais Mudan√ßas nos Dockerfiles

### **Dockerfile.gpu**
```dockerfile
# ANTES: imagem runtime (incompleta)
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# DEPOIS: imagem devel (completa com cuDNN)
FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

# + Configura√ß√£o correta do LD_LIBRARY_PATH
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH

# + PaddlePaddle com vers√£o espec√≠fica para CUDA 11.8
RUN python3 -m pip install paddlepaddle-gpu==2.6.2.post118 \
    -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html
```

### **Dockerfile.gpu-fixed**
```dockerfile
# Mant√©m imagem runtime MAS:
# 1. Procura cuDNN no sistema
# 2. Cria links simb√≥licos automaticamente
# 3. Configura LD_LIBRARY_PATH corretamente
# 4. Adiciona verifica√ß√µes de debug

RUN echo "=== Verificando cuDNN ===" && \
    if [ ! -f /usr/local/cuda/lib64/libcudnn.so ]; then \
        CUDNN_PATH=$(find /usr -name "libcudnn.so*" 2>/dev/null | head -n 1) && \
        if [ -n "$CUDNN_PATH" ]; then \
            CUDNN_DIR=$(dirname "$CUDNN_PATH") && \
            ln -s ${CUDNN_DIR}/libcudnn* /usr/local/cuda/lib64/ 2>/dev/null || true; \
        fi; \
    fi
```

### **server.py melhorado**
```python
# ANTES: Falhava com segfault ao detectar GPU sem cuDNN
def check_gpu_available():
    try:
        paddle.device.set_device('gpu:0')
        return True
    except:
        return False  # ‚ö†Ô∏è Causava segfault!

# DEPOIS: Detecta melhor e for√ßa CPU seguro
def check_gpu_available():
    try:
        # Testa CUDA completamente antes de retornar True
        paddle.device.set_device('gpu:0')
        test_tensor = paddle.ones([1, 1])
        result = paddle.sum(test_tensor)  # Testa opera√ß√£o real
        return True
    except Exception as cuda_err:
        # ‚úÖ CRITICAL: For√ßa CPU para evitar segfault
        paddle.device.set_device('cpu')
        return False
```

---

## üê≥ Deploy no vast.ai

### **1. Fazer push da imagem para Docker Hub**

```bash
# Login no Docker Hub
docker login

# Tag da imagem
docker tag paddleocr-api:gpu SEU_USUARIO/paddleocr-api:gpu-v3

# Push
docker push SEU_USUARIO/paddleocr-api:gpu-v3
```

### **2. Configurar no vast.ai**

Ao criar a inst√¢ncia no vast.ai, use:

```
Image: SEU_USUARIO/paddleocr-api:gpu-v3
Docker Options:
  --gpus all
  -p 8000:8000
  -e PORT=8000
```

### **3. Verificar logs ap√≥s startup**

```bash
# SSH na inst√¢ncia vast.ai
ssh root@SEU_ENDERECO_VASTAI

# Ver logs do container
docker logs -f CONTAINER_ID
```

Voc√™ deve ver:
```
CUDA_VISIBLE_DEVICES: 0
LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:...
‚úÖ GPU detectada! 1 GPU(s) dispon√≠vel(is)
PaddlePaddle version: 2.6.2
Testando inicializa√ß√£o CUDA...
‚úÖ CUDA completamente funcional - usando GPU
   Teste tensor executado com sucesso: Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True, [1.])
‚úÖ Pool de OCR inicializado: 2 inst√¢ncias (GPU - modo SERIAL)
```

---

## üß™ Testar a API

```bash
# Health check
curl http://SEU_IP_VASTAI:8000/health

# Teste de OCR
curl -X POST "http://SEU_IP_VASTAI:8000/ocr/extract" \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://exemplo.com/documento.jpg"]
  }'
```

---

## üîß Troubleshooting

### **Problema: Ainda vendo erro "libcudnn.so not found"**

**Solu√ß√£o:**
1. Use `Dockerfile.gpu-fixed` em vez de `Dockerfile.gpu`
2. Verifique se a imagem base do vast.ai tem cuDNN instalado:
   ```bash
   docker run --rm nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 \
     find /usr -name "libcudnn*"
   ```

### **Problema: Segfault continua acontecendo**

**Solu√ß√£o:**
1. O server.py foi atualizado para evitar isso
2. Reconstrua a imagem com o novo server.py:
   ```bash
   docker build -f Dockerfile.gpu -t paddleocr-api:gpu-v3 .
   ```

### **Problema: Container inicia mas GPU n√£o √© detectada**

**Solu√ß√£o:**
1. Verifique se est√° usando `--gpus all` no docker run
2. Verifique se NVIDIA runtime est√° instalado no vast.ai:
   ```bash
   docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
   ```

### **Problema: "Cannot load cudnn shared library"**

**Solu√ß√£o:**
1. Verifique LD_LIBRARY_PATH dentro do container:
   ```bash
   docker exec CONTAINER_ID env | grep LD_LIBRARY_PATH
   ```
   Deve conter: `/usr/local/cuda/lib64`

2. Verifique se cuDNN existe:
   ```bash
   docker exec CONTAINER_ID ls -la /usr/local/cuda/lib64/libcudnn*
   ```

---

## üìä Diferen√ßas entre Solu√ß√µes

| Aspecto | Dockerfile Original | Dockerfile.gpu | Dockerfile.gpu-fixed |
|---------|-------------------|----------------|---------------------|
| Imagem base | runtime | **devel** | runtime |
| cuDNN | Depende da imagem | ‚úÖ Incluso | ‚úÖ Auto-detecta e instala |
| LD_LIBRARY_PATH | ‚ùå N√£o configurado | ‚úÖ Configurado | ‚úÖ Configurado |
| PaddlePaddle | 2.6.2 (gen√©rico) | 2.6.2.post118 | 2.6.2.post118 |
| Workers | 4 | 2 (GPU) | 2 (GPU) |
| Tamanho imagem | ~2GB | ~4GB | ~2.5GB |
| Confiabilidade | ‚ö†Ô∏è Baixa | ‚úÖ Alta | ‚úÖ Muito Alta |

---

## ‚öôÔ∏è Configura√ß√µes Adicionais

### **Ajustar workers para sua GPU**

No Dockerfile, linha CMD:
```dockerfile
# GPU pequena (< 8GB VRAM): 1-2 workers
CMD gunicorn server:app -w 2 ...

# GPU m√©dia (8-16GB VRAM): 2-3 workers
CMD gunicorn server:app -w 3 ...

# GPU grande (> 16GB VRAM): 4 workers
CMD gunicorn server:app -w 4 ...
```

### **Ajustar mem√≥ria GPU por inst√¢ncia**

No server.py:70-89, ajuste `gpu_mem`:
```python
if use_gpu:
    ocr_config['gpu_mem'] = 4000  # 4GB padr√£o
    # Para GPU com pouca mem√≥ria: 2000-3000
    # Para GPU com muita mem√≥ria: 6000-8000
```

---

## üìù Resumo

1. **Use `Dockerfile.gpu`** se voc√™ tem controle sobre a imagem base (recomendado)
2. **Use `Dockerfile.gpu-fixed`** se precisa de m√°xima compatibilidade
3. **server.py foi melhorado** para evitar segfault automaticamente
4. **Teste localmente primeiro** antes de fazer deploy no vast.ai
5. **Monitore os logs** para confirmar que GPU est√° sendo usada

---

## üÜò Precisa de Ajuda?

Se ainda tiver problemas:

1. Verifique logs completos do container
2. Execute comandos de debug:
   ```bash
   # Dentro do container
   python3 -c "import paddle; print(paddle.__version__); print(paddle.device.cuda.device_count())"

   # Verificar cuDNN
   find /usr -name "libcudnn*" 2>/dev/null

   # Verificar CUDA
   ls -la /usr/local/cuda/lib64/ | grep cudnn
   ```

3. Copie os logs de erro completos para an√°lise mais detalhada
